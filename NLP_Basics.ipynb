{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f550f6b-5592-403d-8a5c-9468ea09b5d0",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5611e8ef-f27b-480b-8e5c-d13997619a07",
   "metadata": {},
   "source": [
    "## Why Do We Need Tokenization?\n",
    "\n",
    "Computers do not understand raw text.\n",
    "\n",
    "Before feeding text into:\n",
    "- Machine Learning models\n",
    "- Language Models (LLMs)\n",
    "- NLP pipelines\n",
    "\n",
    "We must convert text into smaller pieces (tokens) that can be processed numerically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba069a7c-207d-4ab0-bc3b-63a936d2152b",
   "metadata": {},
   "source": [
    "## Load GPT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c4b3ffd-01aa-4bef-aae8-c83141572a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "Playing unbelievably with OpenAI tokenization.\n",
      "\n",
      "Subword Tokens:\n",
      "['Playing', ' unbelievably', ' with', ' Open', 'AI', ' token', 'ization', '.']\n",
      "\n",
      "Token IDs:\n",
      "[41323, 180692, 483, 7788, 17527, 6602, 2860, 13]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Load tokenizer for a GPT model\n",
    "# You can use: \"gpt-4o-mini\", \"gpt-4\", \"gpt-3.5-turbo\"\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "text = \"Playing unbelievably with OpenAI tokenization.\"\n",
    "\n",
    "# Encode text into token IDs\n",
    "token_ids = encoding.encode(text)\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(text)\n",
    "\n",
    "tokens = [encoding.decode([token]) for token in token_ids]\n",
    "\n",
    "print(\"\\nSubword Tokens:\")\n",
    "print(tokens)\n",
    "\n",
    "print(\"\\nToken IDs:\")\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864c6119-fae5-4999-b0c1-5a17c77c7a4a",
   "metadata": {},
   "source": [
    "## Decode Back to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e771cd4-9ef5-4b6a-ac10-1084a832597f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Text:\n",
      "Playing unbelievably with OpenAI tokenization.\n"
     ]
    }
   ],
   "source": [
    "decoded_text = encoding.decode(token_ids)\n",
    "\n",
    "print(\"Decoded Text:\")\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf33ccf-59ab-40cc-ab25-424e7adaf9ce",
   "metadata": {},
   "source": [
    "# ==============================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d565a6a9-7a0a-4bfb-8ac0-47fcb4c93780",
   "metadata": {},
   "source": [
    "# Stop Word Removal in NLP\n",
    "\n",
    "Stop words are common words that usually do not add significant meaning.\n",
    "\n",
    "Examples:\n",
    "- is\n",
    "- the\n",
    "- in\n",
    "- at\n",
    "- on\n",
    "- and\n",
    "\n",
    "Removing stop words helps:\n",
    "- Reduce noise\n",
    "- Reduce dimensionality\n",
    "- Improve NLP model efficiency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b39b84d-93b5-42cc-8cb9-2cad3ab21451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\singazq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\singazq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\singazq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\singazq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\singazq\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b50f876f-0561-451d-8354-88155e80b29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens:\n",
      "['Stop', 'word', 'removal', 'is', 'an', 'important', 'step', 'in', 'Natural', 'Language', 'Processing', '.']\n",
      "\n",
      "Filtered Tokens:\n",
      "['Stop', 'word', 'removal', 'important', 'step', 'Natural', 'Language', 'Processing', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "text = \"Stop word removal is an important step in Natural Language Processing.\"\n",
    "\n",
    "# Tokenize\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Load stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Remove stopwords\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "print(\"Original Tokens:\")\n",
    "print(words)\n",
    "\n",
    "print(\"\\nFiltered Tokens:\")\n",
    "print(filtered_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2ffa6331-7dd4-4793-953e-a389489a147c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e930ee92-4fee-4087-a6f3-8b439c1d9097",
   "metadata": {},
   "source": [
    "# =================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6751d0-090f-4edb-b32a-f51c31c43a32",
   "metadata": {},
   "source": [
    "# Stemming vs Lemmatization Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bbf19cf-25a9-4506-9758-90af37bb4dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words:\n",
      "['The', 'boys', 'are', 'running', 'faster', 'than', 'the', 'girls', '.', 'He', 'studies', 'better', 'than', 'others', '.']\n",
      "Stemmed Words:\n",
      "['the', 'boy', 'are', 'run', 'faster', 'than', 'the', 'girl', '.', 'he', 'studi', 'better', 'than', 'other', '.']\n",
      "Lemmatized Words (without POS):\n",
      "['The', 'boy', 'are', 'running', 'faster', 'than', 'the', 'girl', '.', 'He', 'study', 'better', 'than', 'others', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "text = \"The boys are running faster than the girls. He studies better than others.\"\n",
    "\n",
    "# Tokenize\n",
    "words = word_tokenize(text)\n",
    "\n",
    "print(\"Original Words:\")\n",
    "print(words)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "print(\"Stemmed Words:\")\n",
    "print(stemmed_words)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "print(\"Lemmatized Words (without POS):\")\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354d0376-2127-468a-a00c-407465bc10c1",
   "metadata": {},
   "source": [
    "# ============================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e11f00-c404-4592-a066-11c5be410d73",
   "metadata": {},
   "source": [
    "# Bag of Words\n",
    "\n",
    "Bag of Words is a text representation technique that:\n",
    "\n",
    "- Converts text into numerical vectors\n",
    "- Ignores grammar and word order\n",
    "- Counts word frequency\n",
    "\n",
    "Each unique word becomes a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfa71e98-6d59-4596-a746-b1bd5216f040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "['fun', 'i', 'is', 'learning', 'love', 'machine', 'nlp']\n"
     ]
    }
   ],
   "source": [
    "# Sample sentences\n",
    "documents = [\n",
    "    \"I love NLP\",\n",
    "    \"I love Machine Learning\",\n",
    "    \"NLP is fun\"\n",
    "]\n",
    "\n",
    "# Step 1: Build vocabulary\n",
    "vocab = set()\n",
    "\n",
    "for doc in documents:\n",
    "    for word in doc.lower().split():\n",
    "        vocab.add(word)\n",
    "\n",
    "vocab = sorted(vocab)\n",
    "\n",
    "print(\"Vocabulary:\")\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ea7b658-6d26-4e95-975b-d4631a731f75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words Vectors:\n",
      "\n",
      "I love NLP -> [0, 1, 0, 0, 1, 0, 1]\n",
      "I love Machine Learning -> [0, 1, 0, 1, 1, 1, 0]\n",
      "NLP is fun -> [1, 0, 1, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Create vectors\n",
    "bow_vectors = []\n",
    "\n",
    "for doc in documents:\n",
    "    word_count = []\n",
    "    words = doc.lower().split()\n",
    "    \n",
    "    for word in vocab:\n",
    "        word_count.append(words.count(word))\n",
    "    \n",
    "    bow_vectors.append(word_count)\n",
    "\n",
    "# Display result\n",
    "print(\"Bag of Words Vectors:\\n\")\n",
    "for doc, vector in zip(documents, bow_vectors):\n",
    "    print(f\"{doc} -> {vector}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344b58cb-db1d-4c17-abab-47096e9abe3b",
   "metadata": {},
   "source": [
    "# ======================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde3229a-b8d5-48b8-b71d-04231d4c3ea5",
   "metadata": {},
   "source": [
    "# N-gram Bag of Words\n",
    "\n",
    "N-grams are continuous sequences of N words.\n",
    "\n",
    "Examples:\n",
    "- Unigram (1-gram): single words\n",
    "- Bigram (2-gram): two-word pairs\n",
    "- Trigram (3-gram): three-word sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ce180d8-10cb-4265-9907-c7be02642eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "documents = [\n",
    "    \"I love NLP\",\n",
    "    \"I love Machine Learning\",\n",
    "    \"NLP is fun\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5071c6-a209-4412-8ce4-90510705535f",
   "metadata": {},
   "source": [
    "## Unigram (Standard BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d982ab4-fb73-4776-a9c8-ce2ec5bb4944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Features:\n",
      "['fun' 'is' 'learning' 'love' 'machine' 'nlp']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fun</th>\n",
       "      <th>is</th>\n",
       "      <th>learning</th>\n",
       "      <th>love</th>\n",
       "      <th>machine</th>\n",
       "      <th>nlp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fun  is  learning  love  machine  nlp\n",
       "0    0   0         0     1        0    1\n",
       "1    0   0         1     1        1    0\n",
       "2    1   1         0     0        0    1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_uni = CountVectorizer(ngram_range=(1,1))  # Unigram\n",
    "X_uni = vectorizer_uni.fit_transform(documents)\n",
    "\n",
    "df_uni = pd.DataFrame(X_uni.toarray(), \n",
    "                      columns=vectorizer_uni.get_feature_names_out())\n",
    "\n",
    "print(\"Unigram Features:\")\n",
    "print(vectorizer_uni.get_feature_names_out())\n",
    "df_uni"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2f851a-7ae2-4a15-8453-48521a280ca2",
   "metadata": {},
   "source": [
    "## Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4724432c-300e-49b0-8820-860dccb15fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Features:\n",
      "['is fun' 'love machine' 'love nlp' 'machine learning' 'nlp is']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is fun</th>\n",
       "      <th>love machine</th>\n",
       "      <th>love nlp</th>\n",
       "      <th>machine learning</th>\n",
       "      <th>nlp is</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is fun  love machine  love nlp  machine learning  nlp is\n",
       "0       0             0         1                 0       0\n",
       "1       0             1         0                 1       0\n",
       "2       1             0         0                 0       1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_bi = CountVectorizer(ngram_range=(2,2))  # Bigram\n",
    "X_bi = vectorizer_bi.fit_transform(documents)\n",
    "\n",
    "df_bi = pd.DataFrame(X_bi.toarray(), \n",
    "                     columns=vectorizer_bi.get_feature_names_out())\n",
    "\n",
    "print(\"Bigram Features:\")\n",
    "print(vectorizer_bi.get_feature_names_out())\n",
    "df_bi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa90581-8332-41d4-9f90-39132926ca57",
   "metadata": {},
   "source": [
    "## Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b6bb071-f45b-4d40-88ac-c589d0764375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram Features:\n",
      "['love machine learning' 'nlp is fun']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>love machine learning</th>\n",
       "      <th>nlp is fun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   love machine learning  nlp is fun\n",
       "0                      0           0\n",
       "1                      1           0\n",
       "2                      0           1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_tri = CountVectorizer(ngram_range=(3,3))  # Trigram\n",
    "X_tri = vectorizer_tri.fit_transform(documents)\n",
    "\n",
    "df_tri = pd.DataFrame(X_tri.toarray(), \n",
    "                      columns=vectorizer_tri.get_feature_names_out())\n",
    "\n",
    "print(\"Trigram Features:\")\n",
    "print(vectorizer_tri.get_feature_names_out())\n",
    "df_tri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc46680-9a69-4d58-926c-44891f215d51",
   "metadata": {},
   "source": [
    "# ========================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac9fda2-b156-4263-9eb3-901308f93c98",
   "metadata": {},
   "source": [
    "# TF-IDF (Term Frequency â€“ Inverse Document Frequency)\n",
    "TF-IDF is a technique used in Natural Language Processing (NLP) to convert text into numbers based on how important a word is in a document.\n",
    "\n",
    "TF= Total words in document / Number of times term appears\n",
    "\n",
    "IDF=log(Number of documents containing term / Total number of documents)\n",
    "\n",
    "TF-IDF=TFÃ—IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd54583c-cd6a-44f8-8ce1-61dd1861fb8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amazing</th>\n",
       "      <th>data</th>\n",
       "      <th>is</th>\n",
       "      <th>learning</th>\n",
       "      <th>love</th>\n",
       "      <th>machine</th>\n",
       "      <th>science</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc 1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.622766</td>\n",
       "      <td>0.47363</td>\n",
       "      <td>0.622766</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 3</th>\n",
       "      <td>0.562829</td>\n",
       "      <td>0.428046</td>\n",
       "      <td>0.562829</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        amazing      data        is  learning     love   machine   science\n",
       "Doc 1  0.000000  0.577350  0.000000  0.000000  0.57735  0.000000  0.577350\n",
       "Doc 2  0.000000  0.000000  0.000000  0.622766  0.47363  0.622766  0.000000\n",
       "Doc 3  0.562829  0.428046  0.562829  0.000000  0.00000  0.000000  0.428046"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"I love data science\",\n",
    "    \"I love machine learning\",\n",
    "    \"Data science is amazing\"\n",
    "]\n",
    "\n",
    "# Create TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_tfidf = pd.DataFrame(\n",
    "    X.toarray(),\n",
    "    columns=vectorizer.get_feature_names_out(),\n",
    "    index=[f\"Doc {i+1}\" for i in range(len(documents))]\n",
    ")\n",
    "\n",
    "# Display nicely\n",
    "df_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf5ff1d-8089-4e52-9c2a-2db1746218df",
   "metadata": {},
   "source": [
    "# ==============================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86241576-d028-4e4d-a484-a9bf838b0871",
   "metadata": {},
   "source": [
    "# Part-of-Speech (POS) Tagging\n",
    "POS tagging assigns grammatical labels to words.\n",
    "\n",
    "Examples:\n",
    "- NN  â†’ Noun\n",
    "- VB  â†’ Verb\n",
    "- JJ  â†’ Adjective\n",
    "- RB  â†’ Adverb\n",
    "\n",
    "It helps NLP systems understand grammar and structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea6e2476-c67d-41fa-bd11-84ab86d5b5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\singazq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\singazq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\singazq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c8f5328-a852-4d20-96d5-94b15fcd2f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('boy', 'NN'), ('is', 'VBZ'), ('running', 'VBG'), ('fast', 'RB'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "text = \"The boy is running fast.\"\n",
    "\n",
    "words = word_tokenize(text)\n",
    "tagged_words = pos_tag(words)\n",
    "\n",
    "print(tagged_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbc4abf-ce46-4fd4-a6f5-10675c28ac45",
   "metadata": {},
   "source": [
    "# ===================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601f90eb-9b9d-48f6-94ba-6f55ebd9a186",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER) is an NLP technique that:\n",
    "\n",
    "ðŸ‘‰ Identifies and classifies important entities in text.\n",
    "\n",
    "Common entity types:\n",
    "\n",
    "PERSON â†’ Rahul, Elon Musk\n",
    "\n",
    "ORG â†’ Google, TCS\n",
    "\n",
    "GPE / LOCATION â†’ India, New York\n",
    "\n",
    "DATE â†’ 2024, January\n",
    "\n",
    "MONEY â†’ $500\n",
    "\n",
    "TIME â†’ 10 AM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4584ab2-1739-4d91-9f48-9104fbecd61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c2f03e5-e79c-4543-a360-d79cab6cf3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google -> ORG\n",
      "India -> GPE\n",
      "2022 -> DATE\n"
     ]
    }
   ],
   "source": [
    "text = \"Rahul works at Google in India and joined in 2022.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"->\", ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bf0a11fe-6adc-4531-986b-260113fa66da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple -> ORG\n",
      "Apple -> ORG\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Apple is tasty\"\n",
    "text2 = \"Apple released a new iPhone\"\n",
    "\n",
    "doc = nlp(text1)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"->\", ent.label_)\n",
    "\n",
    "doc = nlp(text2)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"->\", ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1908028-80ef-499a-8566-99114dd68bcf",
   "metadata": {},
   "source": [
    "# ================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1688e8b-b47c-4527-8444-9a260529343e",
   "metadata": {},
   "source": [
    "# BERT (Bidirectional Encoder Representations from Transformers)\n",
    "It revolutionized NLP by understanding context from both directions (left and right).\n",
    "\n",
    "Before BERT:\n",
    "\n",
    "Models read text left â†’ right (like GPT earlier versions)\n",
    "\n",
    "Limited contextual understanding\n",
    "\n",
    "BERT:\n",
    "\n",
    "Reads the whole sentence at once\n",
    "\n",
    "Understands context bidirectionally\n",
    "\n",
    "Example:\n",
    "\n",
    "\"He went to the bank.\"\n",
    "\n",
    "BERT understands:\n",
    "\n",
    "\"bank\" could mean financial institution or river bank\n",
    "\n",
    "It decides based on surrounding words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "658f8ca7-6399-4814-84a8-4caeb78c58b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef46f1f2f6f405a99ede0c5bfefa86f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of last hidden state:\n",
      "torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pretrained BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "text = \"BERT understands context better than older models.\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Get embeddings\n",
    "outputs = model(**inputs)\n",
    "\n",
    "print(\"Shape of last hidden state:\")\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9fab8695-411b-4c9a-a215-b84a51cdbacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bert', 'understands', 'context', 'better', 'than', 'older', 'models', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6db27034-f694-4b13-b553-54f4b0148176",
   "metadata": {},
   "source": [
    "That is 8 tokens.\n",
    "\n",
    "Now BERT automatically adds:\n",
    "\n",
    "[CLS] at the beginning\n",
    "\n",
    "[SEP] at the end\n",
    "\n",
    "So total becomes:\n",
    "\n",
    "[CLS] bert understands context better than older models . [SEP]\n",
    "\n",
    "that is 10 tokens... i.e. \n",
    "\n",
    "1 sentence\n",
    "   â†“\n",
    "10 tokens\n",
    "   â†“\n",
    "Each token â†’ 768 numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e8e5517d-5a76-4493-b11a-4564fdc80b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5566, -0.1176, -0.0310,  ..., -0.3326, -0.0759,  0.5822],\n",
      "         [ 0.2180, -0.1765,  0.1108,  ..., -0.1463,  0.4253,  0.1268],\n",
      "         [-0.7525,  0.4435,  0.0050,  ..., -0.7219, -0.4487,  0.3388],\n",
      "         ...,\n",
      "         [ 0.3207,  0.0362, -0.8701,  ..., -0.6677, -0.2056,  0.2906],\n",
      "         [-0.5575, -0.7512, -0.2402,  ...,  0.3028,  0.3006, -0.2441],\n",
      "         [ 0.6131,  0.3551, -0.4015,  ...,  0.6005, -0.5748, -0.2608]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0f90b09c-6f90-4753-9ddb-934b60ef3543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2a4c6da4-ae7d-49be-ac20-69cea04d5e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs length: 10\n",
      "Tokens: ['[CLS]', 'bert', 'understands', 'context', 'better', 'than', 'older', 'models', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(\"Input IDs length:\", len(inputs[\"input_ids\"][0]))\n",
    "print(\"Tokens:\", tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c367241e-9245-4919-9094-ecdfefad6ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_gemini",
   "language": "python",
   "name": "env_gemini"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
